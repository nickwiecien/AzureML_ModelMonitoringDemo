{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a99ebec9-d3f4-42f0-9279-27d87cffa959",
   "metadata": {},
   "source": [
    "# Azure Machine Learning - Dataset Drift Detection Demo\n",
    "\n",
    "Sample notebook showcasing how to programmatically configure an Azure ML dataset monitor to identify when data has drifted materially. Here, we identify changes in distributions of training & scoring data, and can use these signals to kick off downstream ML model retraining activities to ensure continued model performance. In our example, we train a model for predicting taxi fare using the New York Taxi Cab Green Dataset (from Azure ML's Open Datasets), and store batched results into an Azure ML-linked datastore. Here, we configure our dataset monitor to run weekly to identify drift and can utilize detection to initiate downstream retraining. \n",
    "\n",
    "In terms of using this approach in a MLOps workflow, upon retraining a new model, dataset monitors can be updated so the baseline is inclusive of all training data, and the target dataset contains all inputs/outputs captured moving forward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323a35b8-f221-44ae-8b04-182f7ed2419c",
   "metadata": {},
   "source": [
    "### Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4a90ae-3b7b-4484-80cb-2977db582e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Dataset\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.opendatasets import NycTlcGreen\n",
    "from datetime import datetime\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "from mlflow import set_tracking_uri\n",
    "\n",
    "from sklearn.compose import ColumnTransformer  \n",
    "from sklearn.pipeline import Pipeline  \n",
    "from sklearn.impute import SimpleImputer  \n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder  \n",
    "from sklearn.ensemble import RandomForestRegressor  \n",
    "from sklearn.model_selection import train_test_split "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8124c0c8-a2a4-45d4-aa3e-8215b7c774cb",
   "metadata": {},
   "source": [
    "### Establish connection to Azure ML workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1dc0d3-d2ac-4383-af2c-ea7604ca8795",
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "datastore = ws.get_default_datastore()\n",
    "set_tracking_uri(ws.get_mlflow_tracking_uri())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9798ab08-83ca-4dd9-bb22-df482cef29bd",
   "metadata": {},
   "source": [
    "### Create compute cluster for drift analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c8ba66-6e3a-49e2-943e-09faa6f5ac4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a name for the CPU cluster\n",
    "cpu_cluster_name = \"cpu-cluster\"\n",
    "\n",
    "# Define the configuration for the CPU cluster\n",
    "cpu_config = AmlCompute.provisioning_configuration(vm_size=\"STANDARD_D2_V2\",\n",
    "                                                   min_nodes=0,\n",
    "                                                   max_nodes=4,\n",
    "                                                   idle_seconds_before_scaledown=2400)\n",
    "\n",
    "# Create the CPU cluster\n",
    "cpu_cluster = ComputeTarget.create(ws, cpu_cluster_name, cpu_config)\n",
    "\n",
    "# Monitor the creation process\n",
    "cpu_cluster.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd3b9fd-fc53-4749-9093-5f64c3c64449",
   "metadata": {},
   "source": [
    "### Retrieve New York City Green Taxi Dataset from Azure ML Open Datasets\n",
    "\n",
    "Select a subset of datapoints and register raw dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356b1616-b3bb-4adb-bde8-c55d0029986e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dataset = NycTlcGreen.get_tabular_dataset().take(10000000)\n",
    "df = sample_dataset.to_pandas_dataframe()\n",
    "\n",
    "registered_dataset = Dataset.Tabular.register_pandas_dataframe(\n",
    "    dataframe=df,\n",
    "    name='taxi_data_raw',\n",
    "    description='Sample Data from the NYC Green Taxis Dataset',\n",
    "    target=datastore\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c159303c-f3a4-4b08-ac83-e114d8fe3497",
   "metadata": {},
   "source": [
    "### Downselect to target columns and register separate training & evaluation subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2a13db-d62c-4a35-9f4a-5e67c7bd8f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = ['vendorID', 'lpepPickupDatetime', 'lpepDropoffDatetime', 'passengerCount', 'tripDistance', 'pickupLongitude', 'pickupLatitude', 'dropoffLongitude', 'dropoffLatitude', 'rateCodeID', 'storeAndFwdFlag', 'paymentType', 'fareAmount']\n",
    "\n",
    "df = df[selected_columns]\n",
    "df = df.dropna()\n",
    "\n",
    "df['year'] = df['lpepPickupDatetime'].dt.year\n",
    "df['month'] = df['lpepPickupDatetime'].dt.month\n",
    "df['day'] = df['lpepPickupDatetime'].dt.day\n",
    "df['hour'] = df['lpepPickupDatetime'].dt.hour\n",
    "df['minute'] = df['lpepPickupDatetime'].dt.minute\n",
    "\n",
    "training_df = df[df['month'] <=3]\n",
    "evaluation_df = df[df['month']>3]\n",
    "\n",
    "Dataset.Tabular.register_pandas_dataframe(\n",
    "    dataframe=training_df,\n",
    "    name='taxi_training_data',\n",
    "    description='Training Data from the NYC Green Taxis Dataset',\n",
    "    target=datastore\n",
    ")\n",
    "\n",
    "Dataset.Tabular.register_pandas_dataframe(\n",
    "    dataframe=evaluation_df,\n",
    "    name='taxi_evaluation_data',\n",
    "    description='Evaluation Data from the NYC Green Taxis Dataset',\n",
    "    target=datastore\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb9ece1-0a18-4142-b271-a7b62e11f3cf",
   "metadata": {},
   "source": [
    "### Train and register new Mlflow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ccf9db-89fd-4f84-818b-f9fef7d393f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_dataset = Dataset.get_by_name(ws, 'taxi_training_data')\n",
    "df = taxi_dataset.to_pandas_dataframe()\n",
    "\n",
    "mlflow.autolog(log_input_examples=True, log_model_signatures=True)\n",
    "\n",
    "experiment_name = 'Taxi_Fare_Prediction_Experiment'\n",
    "run_name = 'Random_Forest_Regressor_Trial'\n",
    "\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "run_id = None\n",
    "\n",
    "with mlflow.start_run(run_name=run_name) as run:\n",
    "\n",
    "    # Drop any Datetime columns (try/except)\n",
    "    try:\n",
    "        datetime_cols = [x for x in df.columns.values if 'datetime' in x.lower()]\n",
    "        df.sort_values(by=datetime_cols[0], ascending=True, inplace=True)\n",
    "        training_df = df.sample(100000)\n",
    "        dates = training_df[datetime_cols[0]]\n",
    "        training_df = training_df.drop(columns=datetime_cols)\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "    drop_cols = ['month', 'year']\n",
    "\n",
    "    # Select column types\n",
    "    numeric_features = training_df.drop(columns=['fareAmount']).select_dtypes(include=['int64', 'float64', 'int32']).columns  \n",
    "    categorical_features = training_df.select_dtypes(include=['object']).columns  \n",
    "\n",
    "    # # Define preprocessing for numeric columns (scale them)  \n",
    "    numeric_transformer = Pipeline(steps=[  \n",
    "        ('imputer', SimpleImputer(strategy='median')),  \n",
    "        ('scaler', StandardScaler())])  \n",
    "\n",
    "    # # Define preprocessing for categorical features (encode them)  \n",
    "    categorical_transformer = Pipeline(steps=[  \n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),  \n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))])  \n",
    "\n",
    "    # Combine preprocessing steps  \n",
    "    preprocessor = ColumnTransformer(  \n",
    "        transformers=[  \n",
    "           ('num', numeric_transformer, numeric_features), \n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "    ]) \n",
    "\n",
    "    # Create preprocessing and training pipeline  \n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor),  \n",
    "                               ('regressor', RandomForestRegressor())\n",
    "                              ])  \n",
    "\n",
    "\n",
    "    # Load your data  \n",
    "    X = training_df.drop('fareAmount', axis=1)  \n",
    "    y = training_df['fareAmount']  \n",
    "\n",
    "    # Split your data into train and test datasets  \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)  \n",
    "\n",
    "    # Train model  \n",
    "    pipeline.fit(X_train, y_train) \n",
    "    \n",
    "    run_id = run.info.run_id\n",
    "    \n",
    "model_uri = f'runs:/{run_id}/model'\n",
    "model_name = 'taxi-fare-prediction-model'\n",
    "registered_model = mlflow.register_model(model_uri, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2df3f7c-9673-4d03-8bc9-2f08bbef2d2c",
   "metadata": {},
   "source": [
    "### Score all training data and upload scored dataset to Azure ML-linked datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b975d7-08dc-4c0e-aa7c-cbe815569396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test data  \n",
    "y_pred = pipeline.predict(training_df)  \n",
    "y_pred\n",
    "\n",
    "training_df['Predicted_Fare'] = y_pred\n",
    "training_df['date'] = dates\n",
    "training_df\n",
    "\n",
    "print(max(training_df['date']))\n",
    "max_date = max(training_df['date']).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# save csv, upload and create a dataset\n",
    "filename = './scored-taxi-data-' + max_date + '.csv'\n",
    "training_df.to_csv(filename, index=False)       \n",
    "datastore.upload_files(files=[filename], target_path=\"scored-taxi-data/\", overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b0b288-f778-4b9d-80d6-9debb620d350",
   "metadata": {},
   "source": [
    "### Score all evaluation data and upload scored data to Azure ML-linked datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408da297-a3b8-4e93-8bab-2ce3e707d7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_cols = [x for x in df.columns.values if 'datetime' in x.lower()]\n",
    "dates = evaluation_df[datetime_cols[0]]\n",
    "\n",
    "try:\n",
    "    evaluation_df.sort_values(by=datetime_cols[0], ascending=True, inplace=True)\n",
    "    evaluation_df = evaluation_df.sample(250000)\n",
    "    dates = evaluation_df[datetime_cols[0]]\n",
    "    evaluation_df = evaluation_df.drop(columns=datetime_cols)\n",
    "except Exception as e:\n",
    "    pass\n",
    "\n",
    "preds = pipeline.predict(evaluation_df)\n",
    "\n",
    "evaluation_df['Predicted_Fare'] = preds\n",
    "evaluation_df['date'] = dates\n",
    "evaluation_df\n",
    "\n",
    "print(max(evaluation_df['date']))\n",
    "max_date = max(evaluation_df['date']).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# save csv, upload and create a dataset\n",
    "filename = './scored-taxi-data-' + max_date + '.csv'\n",
    "evaluation_df.to_csv(filename, index=False)       \n",
    "datastore.upload_files(files=[filename], target_path=\"scored-taxi-data/\", overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261a50f5-348d-4f74-93a2-c20a98281138",
   "metadata": {},
   "source": [
    "### Create dataset from CSV files in AML datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e63e59-eed5-4d6b-a1e5-abd3a2f09e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_paths = [(datastore, 'scored-taxi-data/*.csv')] # use wildcard to match all csv files in the folder\n",
    "\n",
    "# create the Tabular dataset with 'state' and 'date' as virtual columns\n",
    "dset = Dataset.Tabular.from_delimited_files(path=csv_paths)\n",
    "\n",
    "# assign the timestamp attribute to a real or virtual column in the dataset\n",
    "dset = dset.with_timestamp_columns('date')\n",
    "\n",
    "# register the dataset as the target dataset\n",
    "dset = dset.register(ws, 'scored_taxi_data', create_new_version=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dcb311-cc99-408b-bca5-332fa065098b",
   "metadata": {},
   "source": [
    "### Configure and run Azure ML dataset monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdea411c-66cf-432e-81c6-a8ff8195b97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Dataset\n",
    "from azureml.datadrift import DataDriftDetector\n",
    "from datetime import datetime\n",
    "\n",
    "# get the target dataset\n",
    "target = Dataset.get_by_name(ws, 'scored_taxi_data')\n",
    "\n",
    "# set the baseline dataset\n",
    "baseline = target.time_before(datetime(2014, 4, 1))\n",
    "\n",
    "# set up feature list\n",
    "\n",
    "# set up data drift detector\n",
    "monitor = DataDriftDetector.create_from_datasets(ws, 'taxi-data-monitor', baseline, target,\n",
    "                                                      compute_target='cpu-cluster',\n",
    "                                                      frequency='Week',\n",
    "                                                      feature_list=None,\n",
    "                                                      drift_threshold=.6,\n",
    "                                                      latency=0)\n",
    "\n",
    "backfill1 = monitor.backfill(datetime(2014, 4, 1), datetime(2014, 10, 1))\n",
    "\n",
    "# update data drift detector\n",
    "monitor = monitor.update(feature_list=['passenger_count', 'trip_distance', 'pickupLongitude', 'pickupLatitude', 'dropoffLongitude', 'dropoffLatitude', 'Predicted_Fare'])\n",
    "\n",
    "monitor.enable_schedule()\n",
    "\n",
    "monitor.run(datetime(2014, 12, 31))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 - AzureML",
   "language": "python",
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
